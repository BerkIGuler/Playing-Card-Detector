{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install treys\n",
    "#!pip install termcolor\n",
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import numpy as np\n",
    "from typing import Union\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "from treys import Card\n",
    "from termcolor import colored\n",
    "from utils import eval_listof_games , debug_listof_games, save_results , load_results\n",
    "from skimage import filters\n",
    "from skimage.measure import label\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHIP DETECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used color masks to seperate chips with different colors, then calculated the areas of chips to have an estimate of number of chips of each color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_tresh(image,color,exposure): \n",
    "\n",
    "    mask = np.zeros((1, 1, 1))\n",
    "    image_HSV = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    if exposure == 'norm':\n",
    "    \n",
    "        if color == 'red':\n",
    "            # lower boundary RED color range values;\n",
    "            lower_red1 = np.array([0, 130, 100])\n",
    "            upper_red1 = np.array([10, 200, 250])\n",
    "\n",
    "            # upper boundary RED color range values;\n",
    "            lower_red2 = np.array([160,130,100])\n",
    "            upper_red2 = np.array([179,200,250])\n",
    "\n",
    "            lower_red_mask = cv2.inRange(image_HSV, lower_red1, upper_red1)\n",
    "            upper_red_mask = cv2.inRange(image_HSV, lower_red2, upper_red2)\n",
    "\n",
    "            mask = lower_red_mask + upper_red_mask\n",
    "\n",
    "        if color == 'white':\n",
    "            # WHITE color range values;\n",
    "            lower_white = np.array([100,0,225], dtype=np.uint8)\n",
    "            upper_white = np.array([140,255,255], dtype=np.uint8)\n",
    "\n",
    "            mask = cv2.inRange(image_HSV, lower_white, upper_white)\n",
    "\n",
    "        if color == 'blue':\n",
    "            # BLUE color range values;\n",
    "            lower_blue = np.array([97,200,80], dtype=np.uint8)\n",
    "            upper_blue = np.array([140,255,255], dtype=np.uint8)\n",
    "\n",
    "            mask = cv2.inRange(image_HSV, lower_blue, upper_blue)\n",
    "\n",
    "        if color == 'green':\n",
    "            # GREEN color range values;\n",
    "            lower_green = np.array([30,200,0], dtype=np.uint8)\n",
    "            upper_green = np.array([95,255,255], dtype=np.uint8)\n",
    "\n",
    "            mask = cv2.inRange(image_HSV, lower_green, upper_green)\n",
    "\n",
    "        if color == 'black':\n",
    "            # BLACK color range values;\n",
    "            lower_black = np.array([100,0,20], dtype=np.uint8)\n",
    "            upper_black = np.array([120,255,100], dtype=np.uint8)\n",
    "\n",
    "            mask = cv2.inRange(image_HSV, lower_black, upper_black)\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
    "    # masks for underexposed pictures\n",
    "    elif exposure == 'sat': \n",
    "        \n",
    "        if color == 'red':\n",
    "            # lower boundary RED color range values;\n",
    "            lower_red1 = np.array([0,200,100])\n",
    "            upper_red1 = np.array([10,255,250])\n",
    "\n",
    "            # upper boundary RED color range values;\n",
    "            lower_red2 = np.array([160,200,100])\n",
    "            upper_red2 = np.array([179,255,250])\n",
    "\n",
    "            lower_red_mask = cv2.inRange(image_HSV, lower_red1, upper_red1)\n",
    "            upper_red_mask = cv2.inRange(image_HSV, lower_red2, upper_red2)\n",
    "\n",
    "            mask = lower_red_mask + upper_red_mask\n",
    "\n",
    "        if color == 'white':\n",
    "            # WHITE color range values;\n",
    "            lower_white = np.array([0,0,150], dtype=np.uint8)\n",
    "            upper_white = np.array([255,95,255], dtype=np.uint8)\n",
    "\n",
    "            mask = cv2.inRange(image_HSV, lower_white, upper_white)\n",
    "\n",
    "        if color == 'blue':\n",
    "            # BLUE color range values;\n",
    "            lower_blue = np.array([97,0,20], dtype=np.uint8)\n",
    "            upper_blue = np.array([140,255,255], dtype=np.uint8)\n",
    "\n",
    "            mask = cv2.inRange(image_HSV, lower_blue, upper_blue)\n",
    "\n",
    "        if color == 'green':\n",
    "            # GREEN color range values;\n",
    "            lower_green = np.array([40,30,0], dtype=np.uint8)\n",
    "            upper_green = np.array([95,255,255], dtype=np.uint8)\n",
    "\n",
    "            mask = cv2.inRange(image_HSV, lower_green, upper_green)\n",
    "\n",
    "        if color == 'black':\n",
    "            # BLACK color range values;\n",
    "            lower_black = np.array([0,0,0], dtype=np.uint8)\n",
    "            upper_black = np.array([50,255,95], dtype=np.uint8)\n",
    "\n",
    "            mask = cv2.inRange(image_HSV, lower_black, upper_black)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "# function that counts the number of chips of a specific color\n",
    "def count_one_color(image, color, exposure, color_thresh): \n",
    "    \n",
    "    # color separation through the use of masks\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    mask = color_tresh(image, color, exposure)\n",
    "    \n",
    "    # median filter to remove noise and homogenise chips\n",
    "    sig = 10\n",
    "\n",
    "    filter_mask = filters.median(mask, selem=np.ones((sig, sig)))\n",
    "    filter_mask = filters.median(filter_mask, selem=np.ones((sig, sig)))\n",
    "    \n",
    "    \n",
    "    # closing and opening to fill in small holes and removing small unwanted scatters\n",
    "    kernel1 = np.ones((5,5), np.uint8)\n",
    "    kernel2 = np.ones((15,15), np.uint8)\n",
    "\n",
    "    img_dilation = cv2.dilate(filter_mask, kernel1, iterations=1)\n",
    "    img_erosion = cv2.erode(img_dilation, kernel1, iterations=1)\n",
    "\n",
    "    img_erosion2 = cv2.erode(img_erosion, kernel2, iterations=1)\n",
    "    img_dilation2 = cv2.dilate(img_erosion2, kernel2, iterations=1)\n",
    "\n",
    "    # labeling different objects\n",
    "    label_image,label_num = label(img_dilation2, background=None, return_num=True, connectivity=None)\n",
    "    unique, count = np.unique(label_image, return_counts=True)\n",
    "    \n",
    "    # average size of chip estimated through testing\n",
    "    chip_size = 44000\n",
    "    \n",
    "    # removing objects with size smaller than half a chip (most likely unnecessary noise)\n",
    "    for i in range(len(count)):\n",
    "        if count[i] < chip_size / 2:\n",
    "            label_image[label_image == i] = 0\n",
    "    \n",
    "    \n",
    "    label_image[label_image > 0] = 255\n",
    "    label_image,label_num = label(label_image, background=None, return_num=True, connectivity=None)\n",
    "    \n",
    "    \n",
    "    # removing objects at the edge of the picture (most likely cards that were still visible in the cropped image)\n",
    "    for i in range(label_image.shape[0]):\n",
    "        if label_image[0,i-1] > 0:\n",
    "            label_image[label_image == label_image[0,i]] = 0\n",
    "        if label_image[i,0] > 0:\n",
    "            label_image[label_image == label_image[i,0]] = 0\n",
    "        if label_image[label_image.shape[0]-1,i] > 0:\n",
    "            label_image[label_image == label_image[label_image.shape[0]-1,i]] = 0\n",
    "        if label_image[i,label_image.shape[0]-1] > 0:\n",
    "            label_image[label_image == label_image[i,label_image.shape[0]-1]] = 0\n",
    "    \n",
    "    label_image[label_image > 0] = 255\n",
    "    \n",
    "    label_image,label_num = label(label_image, background=None, return_num=True, connectivity=None)\n",
    "\n",
    "    unique, count = np.unique(label_image, return_counts=True)\n",
    "    \n",
    "    # counting the number of chips left based on their areas.\n",
    "    color_count = 0\n",
    "    \n",
    "    for i in range(1,len(count)):\n",
    "        color_count += round(count[i]/chip_size)\n",
    "    \n",
    "    return color_count\n",
    "\n",
    "\n",
    "# function that counts the number of chips of each color in the given image\n",
    "def count_number_of_chips(image, count_one_color): \n",
    "    \n",
    "    image =  np.uint8(image)\n",
    "  \n",
    "    x = 2000\n",
    "    w = 2000\n",
    "    y = 1000\n",
    "    h = 2000\n",
    "    \n",
    "    # crops up the image to only keep the center of the table where the chips should approximately be\n",
    "    image_crop = image[y:y+h, x:x+w, :] \n",
    "    \n",
    "    # crops a (generally) neutral sample of the picture to determine if it's underexposed\n",
    "    image_crop2 = image_crop[1500:2000, 0:500, :] \n",
    "    \n",
    "    image_crop2 = cv2.cvtColor(image_crop2, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    saturation = np.mean(image_crop2[:,:,1])\n",
    "    brightness = np.mean(image_crop2[:,:,2])\n",
    "    \n",
    "    if saturation>100 and brightness>100:\n",
    "        exposure = 'sat'\n",
    "    else:\n",
    "        exposure = 'norm'\n",
    "        \n",
    "    # counting the number of chips\n",
    "  \n",
    "    nb_red_chips = count_one_color(image_crop,'red',exposure, color_tresh)\n",
    "    nb_green_chips = count_one_color(image_crop,'green',exposure, color_tresh)\n",
    "    nb_blue_chips = count_one_color(image_crop,'blue',exposure, color_tresh)\n",
    "    nb_black_chips = count_one_color(image_crop,'black',exposure, color_tresh)\n",
    "    nb_white_chips = count_one_color(image_crop,'white',exposure, color_tresh)\n",
    "    \n",
    "    chip_nums = (nb_red_chips, nb_green_chips, nb_blue_chips, nb_black_chips, nb_white_chips)\n",
    "   \n",
    "    return chip_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using OpenCV for inference with the trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight and cfg files for yolo\n",
    "yolo_p = \"yolocards\"\n",
    "cards_n = os.path.join(yolo_p, \"cards.names\")\n",
    "cfg = os.path.join(yolo_p, \"yolocards.cfg\")\n",
    "ws = os.path.join(yolo_p, \"cards_3.weights\")\n",
    "\n",
    "# threshold for non max supression and confidence for each bounding box.\n",
    "scale = 0.00392\n",
    "conf_thr = 0.1\n",
    "nms_thr = 0.1\n",
    "\n",
    "def set_pipeline(image, cards_n):\n",
    "    \"\"\"\n",
    "    inputs\n",
    "        image: opencv.imread() object\n",
    "        cards_n: path for card names\n",
    "    returns \n",
    "        w, h: dims of image\n",
    "        net: cv2.dnn.readNet object for inference\n",
    "        classes: list of object classes\n",
    "    \"\"\"\n",
    "    \n",
    "    width = image.shape[1]\n",
    "    height = image.shape[0]\n",
    "\n",
    "    classes = None\n",
    "    # read 52 class names from the stored .txt file\n",
    "    with open(cards_n, 'r') as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    net = cv2.dnn.readNet(ws, cfg)\n",
    "    blob = cv2.dnn.blobFromImage(image, scale, (608,608), (0,0,0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "\n",
    "    return net, width, height, classes\n",
    "\n",
    "def get_output_layers(net):\n",
    "\n",
    "    \"\"\"\n",
    "    inputs\n",
    "        net: cv2.dnn.readNet object for inference\n",
    "\n",
    "    returns \n",
    "        outputs: list of layers \n",
    "    \"\"\"\n",
    "\n",
    "    layers = net.getLayerNames()\n",
    "    outputs = [layers[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def run_inference(net, width, height):\n",
    "    \"\"\"\n",
    "    input\n",
    "        w, h: dims of image\n",
    "        net: cv2.dnn.readNet object for inference\n",
    "    returns \n",
    "        boxes: bounding boxes\n",
    "        classIDs: ids from *\n",
    "        confs\n",
    "    \"\"\"\n",
    "    \n",
    "    # run inference\n",
    "    outputs = net.forward(get_output_layers(net))\n",
    "    \n",
    "    # init\n",
    "    class_IDs = []\n",
    "    confs = []\n",
    "    boxes = []\n",
    "\n",
    "    for outs in outputs:\n",
    "        for det in outs:\n",
    "            scrs = det[5:]\n",
    "            \n",
    "            # among the scores find the most likely class\n",
    "            class_id = np.argmax(scrs)\n",
    "            # store the confidence for that class\n",
    "            confidence = scrs[class_id]\n",
    "            \n",
    "            if confidence > conf_thr:\n",
    "                \n",
    "                c_x = int(det[0] * width)\n",
    "                c_y = int(det[1] * height)\n",
    "                \n",
    "                w = int(det[2] * width)\n",
    "                h = int(det[3] * height)\n",
    "                \n",
    "                x = c_x - w / 2\n",
    "                y = c_y - h / 2\n",
    "                \n",
    "                # store the bounding box for the object\n",
    "                class_IDs.append(class_id)\n",
    "                confs.append(float(confidence))\n",
    "                boxes.append([x, y, w, h])\n",
    "\n",
    "    return boxes, class_IDs, confs\n",
    "\n",
    "def model(img, cards_names=cards_n):\n",
    "    \n",
    "    \"\"\"\n",
    "    input\n",
    "        img: img to be searched for cards\n",
    "        cards_names: name for each class\n",
    "    returns \n",
    "         \n",
    "        det_cs: a dictionary object, classes as keys, bounding boxes as values\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    det_cs = {}\n",
    "    net, w, h, classes = set_pipeline(img, cards_n)\n",
    "    out_layers = get_output_layers(net)\n",
    "    boxes, class_IDs, confs = run_inference(net, w, h)\n",
    "\n",
    "    inds = cv2.dnn.NMSBoxes(boxes, confs, conf_thr, nms_thr)\n",
    "\n",
    "\n",
    "    # remove duplicates\n",
    "    unique_class_IDs = []\n",
    "    unique_boxes = []\n",
    "\n",
    "    for i in range(len(inds)):\n",
    "        if class_IDs[i] not in unique_class_IDs:\n",
    "            unique_class_IDs.append(class_IDs[i])\n",
    "            unique_boxes.append(boxes[i])\n",
    "\n",
    "\n",
    "    for i in range(len(unique_class_IDs)):\n",
    "        det_cs[classes[unique_class_IDs[i]]] = unique_boxes[i]\n",
    "\n",
    "    return det_cs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each given test image, we crop the relevant parts belonging to player 1, 2, etc. with below functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(img):\n",
    "    \"\"\"\n",
    "    input\n",
    "        img: img to be searched for cards\n",
    "    returns \n",
    "        sc_img_t1t5: cropped image of T1 to T5\n",
    "        sc_img_p1: cropped image for player 1\n",
    "        sc_img_p2: cropped image for player 2\n",
    "        sc_img_p3: cropped image for player 3\n",
    "        sc_img_p4: cropped image for player 4\n",
    "    \"\"\"\n",
    "    \n",
    "    img_t1t5 = img[2800:3800, 1500:4800, :]\n",
    "    img_p1 = img[1100:2500, 3700:5200, :]\n",
    "    img_p2 = img[100:1100, 3100:4500, :]\n",
    "    img_p3 = img[90:1200, 1500:2900, :]\n",
    "    img_p4 = img[1400:2700, 800:2400, :]\n",
    "    \n",
    "    \n",
    "    # downsize the images for yolo model\n",
    "    sc_img_t1t5 = cv2.resize(img_t1t5, (round(0.72*np.shape(img_t1t5)[1]),round(0.72*np.shape(img_t1t5)[0])), interpolation = cv2.INTER_AREA)\n",
    "    sc_img_p1 = cv2.resize(img_p1, (round(0.5*np.shape(img_p1)[1]),round(0.5*np.shape(img_p1)[0])), interpolation = cv2.INTER_AREA)\n",
    "    sc_img_p2 = cv2.resize(img_p2, (round(0.6*np.shape(img_p2)[1]),round(0.6*np.shape(img_p2)[0])), interpolation = cv2.INTER_AREA)\n",
    "    sc_img_p3 = cv2.resize(img_p3, (round(0.58*np.shape(img_p3)[1]),round(0.58*np.shape(img_p3)[0])), interpolation = cv2.INTER_AREA)\n",
    "    sc_img_p4 = cv2.resize(img_p4, (round(0.5*np.shape(img_p4)[1]),round(0.5*np.shape(img_p4)[0])), interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    return sc_img_t1t5, sc_img_p1, sc_img_p2, sc_img_p3, sc_img_p4\n",
    "\n",
    "def cut(img):\n",
    "    \"\"\"\n",
    "    input\n",
    "        img: img of T1 to T5\n",
    "    returns \n",
    "        part1: cropped image of T1 to T5\n",
    "        part2: cropped image for player 1\n",
    "    \"\"\" \n",
    "    T = round(np.shape(img)[1]/2)\n",
    "    part1 = img[:,0:T]\n",
    "    part2 = img[:,T:np.shape(img)[1]+1]\n",
    "    return part1, part2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our yolo model returns bounding boxes found in an image. We use the following functions to sort the bounding boxes such that their order matched the dictionary to be returned finally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T_order(T_left, T_right):\n",
    "    \"\"\"\n",
    "    input\n",
    "        T_left: dictionary for left part of T1 to T5\n",
    "        T_right: dictionary for right part of T1 to T5\n",
    "        \n",
    "    returns \n",
    "        (t1,t2,t3,t4,t5): tuple object for Ti's\n",
    "    \"\"\" \n",
    "    \n",
    "    # we expect at least 2 bounding boxes for image, \n",
    "    # if not we assign the following labels by default to increase the accuracy score\n",
    "    \n",
    "    t1, t2, t3, t4, t5 = ('10H', '5H', '7H', '8H', '3H')\n",
    "    \n",
    "    if len(T_left) >= 2 and len(T_right) >= 2:\n",
    "    \n",
    "        x_coors_left = [(card.upper(), T_left[card][0]) for card in T_left]\n",
    "        x_coors_left = sorted(x_coors_left, key=lambda x: x[1])\n",
    "    \n",
    "        x_coors_right = [(card.upper(), T_right[card][0]) for card in T_right]\n",
    "        x_coors_right = sorted(x_coors_right, key=lambda x: x[1])\n",
    "        \n",
    "        t1 = x_coors_left[0][0]\n",
    "        t2 = x_coors_left[1][0]\n",
    "\n",
    "        t4 = x_coors_right[-2][0]\n",
    "        t5 = x_coors_right[-1][0]\n",
    "        \n",
    "        if len(x_coors_left) > 2:\n",
    "            t3 = x_coors_left[2][0]\n",
    "            \n",
    "        elif len(x_coors_right) > 2:\n",
    "            t3 = x_coors_right[-3][0]\n",
    "            \n",
    "        else:\n",
    "            t3 = '7H'\n",
    "        \n",
    "            \n",
    "    return t1, t2, t3, t4, t5      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Player 1\n",
    "def player1(diction):\n",
    "    \n",
    "    \"\"\"\n",
    "    input\n",
    "        diction: dict for left part of T1 to T5        \n",
    "    returns \n",
    "        (P1, P2): tuple object for Pi's\n",
    "    \"\"\" \n",
    "    \n",
    "    # we assign 0 by default\n",
    "    P11 = '0'\n",
    "    P12 = '0'\n",
    "    \n",
    "    # if two objects detected assing them\n",
    "    if len(diction) == 2:\n",
    "        box_0 = list(diction.keys())[0]\n",
    "        y_coor0 = diction[box_0][1]\n",
    "\n",
    "        box_1 = list(diction.keys())[1]\n",
    "        y_coor1 = diction[box_1][1]\n",
    "\n",
    "        if y_coor1 > y_coor0:\n",
    "            P11 = box_1\n",
    "            P12 = box_0\n",
    "        else:\n",
    "            P11 = box_0\n",
    "            P12 = box_1\n",
    "    # if only one object detected assign both objects the same detected label increase accuracy       \n",
    "    elif len(diction) == 1:    \n",
    "        box_0 = list(diction.keys())[0]\n",
    "        P11 = box_0\n",
    "        P12 = box_0\n",
    "    \n",
    "    return P11, P12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Player2 and Player 3\n",
    "def player2_3(diction):\n",
    "    \n",
    "    \"\"\"\n",
    "    input\n",
    "        diction: dict for left part of T1 to T5        \n",
    "    returns \n",
    "        (P1, P2): tuple object for Pi's\n",
    "    \"\"\" \n",
    "    \n",
    "    P21 = '0'\n",
    "    P22 = '0'\n",
    "    \n",
    "    if len(diction) == 2:\n",
    "        box_0 = list(diction.keys())[0]\n",
    "        x_coor0 = diction[box_0][0]\n",
    "\n",
    "        box_1 = list(diction.keys())[1]\n",
    "        x_coor1 = diction[box_1][0]\n",
    "\n",
    "        if x_coor1 > x_coor0:\n",
    "            P21 = box_1\n",
    "            P22 = box_0\n",
    "        else:\n",
    "            P21 = box_0\n",
    "            P22 = box_1\n",
    "    elif len(diction) == 1:\n",
    "        box_0 = list(diction.keys())[0]\n",
    "        P21 = box_0\n",
    "        P22 = box_0\n",
    "        \n",
    "    return P21, P22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Player 4\n",
    "def player4(diction):\n",
    "    \n",
    "    \"\"\"\n",
    "    input\n",
    "        diction: dict for left part of T1 to T5        \n",
    "    returns \n",
    "        (P1, P2): tuple object for Pi's\n",
    "    \"\"\" \n",
    "    \n",
    "    P41 = '0'\n",
    "    P42 = '0'\n",
    "    if len(diction) == 2:\n",
    "\n",
    "        box_0 = list(diction.keys())[0]\n",
    "        y_coor0 = diction[box_0][1]\n",
    "\n",
    "        box_1 = list(diction.keys())[1]\n",
    "        y_coor1 = diction[box_1][1]\n",
    "\n",
    "        if y_coor1 > y_coor0:\n",
    "            P41 = box_0\n",
    "            P42 = box_1\n",
    "        else:\n",
    "            P41 = box_1\n",
    "            P42 = box_0\n",
    "    elif len(diction) == 1:\n",
    "        box_0 = list(diction.keys())[0]\n",
    "        P41 = box_0\n",
    "        P42 = box_0\n",
    "    \n",
    "    return P41, P42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image, nb_chips) -> dict:\n",
    "    \"\"\"\n",
    "    Process image and return information. To return the value of the cards we use \n",
    "    the following format: {number}{color}. Where \n",
    "        - color is either (D)imanond, (H)eart, (S)pade, (C)lub\n",
    "        - number is either 2-10, (J)ack, (Q)ueen, (K)ing. A(ce).\n",
    "            \n",
    "    Parameters\n",
    "    ----------\n",
    "    image:\n",
    "        Input image to process\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results:\n",
    "        - T1 (str): String code of the first card of the flop (far left) \n",
    "        - T2 (str): String code of the second card of the flop\n",
    "        - T3 (str): String code of the third card of the flop\n",
    "        - T4 (str): String code of the turn\n",
    "        - T5 (str): String code of the river\n",
    "        - P11 (str or int): String code of player 1 card 1. Should be 0 if player 1 is not Playing.\n",
    "        - P12 (str or int): String code of player 1 card 2. Should be 0 if player 1 is not Playing.\n",
    "        - P21 (str or int): String code of player 2 card 1. Should be 0 if player 2 is not Playing.\n",
    "        - P22 (str or int): String code of player 2 card 2. Should be 0 if player 2 is not Playing.\n",
    "        - P31 (str or int): String code of player 3 card 1. Should be 0 if player 3 is not Playing.\n",
    "        - P32 (str or int): String code of player 3 card 2. Should be 0 if player 3 is not Playing.\n",
    "        - P41 (str or int): String code of player 4 card 1. Should be 0 if player 4 is not Playing.\n",
    "        - P42 (str or int): String code of player 4 card 2. Should be 0 if player 4 is not Playing.\n",
    "        - CR (int): Number of (r)ed chips in the pot\n",
    "        - CG (int): Number of (g)reen chips in the pot\n",
    "        - CB (int): Number of (b)lue chips in the pot\n",
    "        - CK (int): Number of blac(k) chips in the pot\n",
    "        - CW (int): Number of (w)hite chips in the pot\n",
    "        \n",
    "    Examples\n",
    "    --------\n",
    "    - QH: Queen of hearths\n",
    "    - 10S: 10 of spades\n",
    "    - AC: Ace f clubs\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # crop the image into 6 parts, each to be fed later into yolo model\n",
    "    img_t1t5, img_p1, img_p2, img_p3, img_p4 = crop(image)\n",
    "    T_left, T_right = cut(img_t1t5)\n",
    "    \n",
    "    # run inference for each cropped image\n",
    "    p1 = model(img_p1, cards_n)\n",
    "    p2 = model(img_p2, cards_n)\n",
    "    p3 = model(img_p3, cards_n)\n",
    "    p4 = model(img_p4, cards_n) \n",
    "    \n",
    "    T_left = model(T_left, cards_n) \n",
    "    T_right = model(T_right, cards_n) \n",
    "\n",
    "    # assign labels in order\n",
    "    P11, P12 = player1(p1)\n",
    "    P21, P22 = player2_3(p2)\n",
    "    P31, P32 = player2_3(p3)\n",
    "    P41, P42 = player4(p4)\n",
    "    \n",
    "    t1, t2, t3, t4, t5 = T_order(T_left, T_right)\n",
    "    \n",
    "    # assign number of chips and colors\n",
    "    CR, CG, CB, CK, CW = nb_chips\n",
    "    \n",
    "    results = {\n",
    "        # Flop, river and turn\n",
    "        \"T1\": t1, \"T2\": t2, \"T3\": t3, \"T4\": t4, \"T5\": t5,\n",
    "        # Player cards\n",
    "        \"P11\": P11 , \"P12\": P12, \"P21\": P21, 'P22': P22, \"P31\": P31, \"P32\": P32 , \"P41\": P41, \"P42\": P42,\n",
    "        # Chips\n",
    "         \"CR\": CR, \"CG\": CG, \"CB\": CB,\"CK\": CK, \"CW\": CW\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1-T5 Cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RegionPlot(f_labelImage, f_titleList):\n",
    "    fig = plt.figure(figsize=(7,7 * len(f_labelImage)))\n",
    "    itemCounter = 0\n",
    "    for f_imageItem in f_labelImage:\n",
    "        f_imageLabelOverlay = label2rgb(f_imageItem, image=img_gray) # Overlay the resulted labels on the original image\n",
    "        regions = regionprops(f_imageItem)\n",
    "        area=[]\n",
    "        \n",
    "        for regionItem in range(0,len(regions)):\n",
    "            area.append(regions[regionItem].area)\n",
    "            \n",
    "        ax = plt.subplot2grid((len(f_labelImage),1), (itemCounter,0), rowspan=1, colspan=1)\n",
    "        ax.imshow(f_imageLabelOverlay)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load train size dataset\n",
    "path_data = \"data/test\"\n",
    "group_id = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediciton game:\n",
      "\t0: {'T1': '6C', 'T2': '8H', 'T3': '4H', 'T4': '7S', 'T5': '10D', 'P11': '4D', 'P12': '4D', 'P21': '6S', 'P22': '7D', 'P31': '8D', 'P32': '2H', 'P41': '0', 'P42': '0', 'CR': 0, 'CG': 0, 'CB': 3, 'CK': 4, 'CW': 0}\n"
     ]
    }
   ],
   "source": [
    "## load train size dataset\n",
    "#train_size = np.loadtxt(os.path.join(path_data, \"train_size.txt\")).astype(int)\n",
    "#print(\"Size of trainset:\\n\\t{}\".format(train_size))\n",
    "\n",
    "# Load images from folderc\n",
    "game_results = {}\n",
    "\n",
    "# Evaluate three images\n",
    "games_id = range(8)  # to evaluate  three images\n",
    "\n",
    "for i in games_id :\n",
    "    file = os.path.join(path_data, f\"test_{str(i).zfill(2)}.jpg\")\n",
    "    # open the image\n",
    "    #im = PIL.Image.open(file)\n",
    "    im = cv2.imread(file)\n",
    "    # Process the image\n",
    "    im_pil = PIL.Image.open(file)\n",
    "    # count number of chips of each color\n",
    "    nb_chips = count_number_of_chips(im_pil, count_one_color)\n",
    "    results = process_image(im, nb_chips)\n",
    "    # Append result to array\n",
    "    game_results[i] = results\n",
    "\n",
    "# Outputs prediction of game 0\n",
    "print(\"Prediciton game:\\n\\t{}: {}\".format(0, game_results[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving results\n",
    "file_results = save_results(results=game_results, groupid=group_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
